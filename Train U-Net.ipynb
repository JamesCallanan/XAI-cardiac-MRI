{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_unet\n",
    "from batchgenerators.dataloading import DataLoaderBase\n",
    "import numpy as np\n",
    "from batchgenerators.augmentations.utils import resize_image_by_padding, random_crop_3D_image_batched, \\\n",
    "    random_crop_2D_image_batched, center_crop_3D_image_batched\n",
    "\n",
    "# source -> https://github.com/MIC-DKFZ/ACDC2017\n",
    "class BatchGenerator_2D(DataLoaderBase):\n",
    "    def __init__(self, data, BATCH_SIZE, PATCH_SIZE=(256, 256), num_batches=None, seed=False):\n",
    "        self.PATCH_SIZE = PATCH_SIZE\n",
    "        DataLoaderBase.__init__(self, data, BATCH_SIZE, num_batches=num_batches, seed=seed)\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        data = np.zeros((self.BATCH_SIZE, 1, self.PATCH_SIZE[0], self.PATCH_SIZE[1]), dtype=np.float32)\n",
    "        seg = np.zeros((self.BATCH_SIZE, 1, self.PATCH_SIZE[0], self.PATCH_SIZE[1]), dtype=np.float32)\n",
    "        types = np.random.choice(['ed', 'es'], self.BATCH_SIZE, True)\n",
    "        patients = np.random.choice(list(self._data.keys()), self.BATCH_SIZE, True)\n",
    "        pathologies = []\n",
    "        for nb in range(self.BATCH_SIZE):\n",
    "            shp = self._data[patients[nb]][types[nb]+'_data'].shape\n",
    "            slice_id = np.random.choice(shp[0])\n",
    "            tmp_data = resize_image_by_padding(self._data[patients[nb]][types[nb]+'_data'][slice_id], (max(shp[1],\n",
    "                                                self.PATCH_SIZE[0]), max(shp[2], self.PATCH_SIZE[1])), pad_value=0)\n",
    "            tmp_seg = resize_image_by_padding(self._data[patients[nb]][types[nb]+'_gt'][slice_id], (max(shp[1],\n",
    "                                                self.PATCH_SIZE[0]), max(shp[2], self.PATCH_SIZE[1])), pad_value=0)\n",
    "\n",
    "            # not the most efficient way but whatever...\n",
    "            tmp = np.zeros((1, 2, tmp_data.shape[0], tmp_data.shape[1]))\n",
    "            tmp[0, 0] = tmp_data\n",
    "            tmp[0, 1] = tmp_seg\n",
    "            tmp = random_crop_2D_image_batched(tmp, self.PATCH_SIZE)\n",
    "            data[nb, 0] = tmp[0, 0]\n",
    "            seg[nb, 0] = tmp[0, 1]\n",
    "            pathologies.append(self._data[patients[nb]]['pathology'])\n",
    "        return {'data':data, 'seg':seg, 'types':types, 'patient_ids': patients, 'pathologies':pathologies}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchgenerators.dataloading import MultiThreadedAugmenter, SingleThreadedAugmenter\n",
    "from batchgenerators.transforms import Compose, RndTransform\n",
    "from batchgenerators.transforms import SpatialTransform\n",
    "from batchgenerators.transforms import MirrorTransform\n",
    "\n",
    "from batchgenerators.transforms import GammaTransform, ConvertSegToOnehotTransform\n",
    "from batchgenerators.transforms import RandomCropTransform\n",
    "\n",
    "\n",
    "# source -> https://github.com/MIC-DKFZ/ACDC2017\n",
    "def create_data_gen_train(patient_data_train, BATCH_SIZE, PATCH_SIZE, NUM_BATCHES, num_classes,\n",
    "                                  num_workers=1, num_cached_per_worker=2,\n",
    "                                  do_elastic_transform=False, alpha=(0., 1300.), sigma=(10., 13.),\n",
    "                                  do_rotation=False, a_x=(0., 2*np.pi), a_y=(0., 2*np.pi), a_z=(0., 2*np.pi),\n",
    "                                  do_scale=True, scale_range=(0.75, 1.25), seeds=None):\n",
    "    if seeds is None:\n",
    "        seeds = [None]*num_workers\n",
    "    elif seeds == 'range':\n",
    "        seeds = range(num_workers)\n",
    "    else:\n",
    "        assert len(seeds) == num_workers\n",
    "    data_gen_train = BatchGenerator_2D(patient_data_train, BATCH_SIZE, num_batches=NUM_BATCHES, seed=False,\n",
    "                                       PATCH_SIZE=PATCH_SIZE)\n",
    "\n",
    "    tr_transforms = []\n",
    "    tr_transforms.append(MirrorTransform((0, 1)))\n",
    "    tr_transforms.append(RndTransform(SpatialTransform(PATCH_SIZE, list(np.array(PATCH_SIZE)//2),\n",
    "                                                       do_elastic_transform, alpha,\n",
    "                                                       sigma,\n",
    "                                                       do_rotation, a_x, a_y,\n",
    "                                                       a_z,\n",
    "                                                       do_scale, scale_range, 'constant', 0, 3, 'constant',\n",
    "                                                       0, 0,\n",
    "                                                       random_crop=False), prob=0.67,\n",
    "                                      alternative_transform=RandomCropTransform(PATCH_SIZE)))\n",
    "    tr_transforms.append(ConvertSegToOnehotTransform(range(num_classes), seg_channel=0, output_key='seg_onehot'))\n",
    "\n",
    "    #tr_composed = Compose(tr_transforms)\n",
    "    tr_composed = Compose([ConvertSegToOnehotTransform(range(num_classes), seg_channel=0, output_key='seg_onehot')])\n",
    "\n",
    "#     tr_mt_gen = MultiThreadedAugmenter(data_gen_train, tr_composed, num_workers, num_cached_per_worker, seeds)\n",
    "#     tr_mt_gen.restart()\n",
    "    tr_mt_gen = SingleThreadedAugmenter(data_gen_train, tr_composed) \n",
    "    return tr_mt_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = {\n",
    "    'BATCH_SIZE': 4,\n",
    "    'INPUT_PATCH_SIZE': (348, 348), # calculated using notebook: Calculate-U-Net Size\n",
    "    'num_classes': 4,\n",
    "    'EXPERIMENT_NAME': 'Test-Implementation',\n",
    "    'results_dir': './results-experiment',\n",
    "    'n_epochs': 5,\n",
    "    'lr_decay': np.float32(0.985),\n",
    "    'base_lr': np.float32(0.0005),\n",
    "    'n_batches_per_epoch': 10,\n",
    "    'n_test_batches': 10,\n",
    "    'n_feedbacks_per_epoch': 10,\n",
    "    'num_workers': 6,\n",
    "    'workers_seeds': [123, 1234, 12345, 123456, 1234567, 12345678],\n",
    "    'weight_decay': 1e-5,\n",
    "    'n_input_channels': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BATCH_SIZE = cf['BATCH_SIZE']\n",
    "INPUT_PATCH_SIZE = cf['INPUT_PATCH_SIZE']\n",
    "num_classes = cf['num_classes']\n",
    "EXPERIMENT_NAME = cf['EXPERIMENT_NAME']\n",
    "results_dir = cf['results_dir']\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "n_epochs = cf['n_epochs']\n",
    "lr_decay = cf['lr_decay']\n",
    "base_lr = cf['base_lr']\n",
    "n_batches_per_epoch = cf['n_batches_per_epoch']\n",
    "n_test_batches = cf['n_test_batches']\n",
    "n_feedbacks_per_epoch = cf['n_feedbacks_per_epoch']\n",
    "num_workers = cf['num_workers']\n",
    "workers_seeds = cf['workers_seeds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from loss import dice_loss\n",
    "\n",
    "# source -> https://github.com/usuyama/pytorch-unet\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    #print(target)\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        \n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# source -> https://github.com/usuyama/pytorch-unet\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "\n",
    "# adapted from -> https://github.com/usuyama/pytorch-unet    \n",
    "def train_model(model, optimizer, scheduler, dataloaders, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for i, data_dict in enumerate(dataloaders[phase]):\n",
    "                \n",
    "                inputs = torch.from_numpy(data_dict['data']).to(device)\n",
    "                types = data_dict['types']\n",
    "                patient_ids = data_dict['patient_ids']\n",
    "                pathologies = data_dict['pathologies']\n",
    "                seg_onehot = data_dict['seg_onehot']\n",
    "                labels = torch.from_numpy(data_dict['seg_onehot']).to(device) # we need an output encoded into 4 layers          \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    labels = labels[:,:,92:INPUT_PATCH_SIZE[0]-92,92:INPUT_PATCH_SIZE[1]-92]\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "                    if i % 250 == 0:\n",
    "                        print(loss)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "            if phase == 'train':\n",
    "                train_history.append(epoch_loss)\n",
    "            elif phase == 'val':\n",
    "                val_history.append(epoch_loss)            \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# source -> https://github.com/MIC-DKFZ/ACDC2017\n",
    "def get_split(fold, seed=12345):\n",
    "    # this is seeded, will be identical each time\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    all_keys = np.arange(1, 101)\n",
    "    splits = kf.split(all_keys)\n",
    "    for i, (train_idx, test_idx) in enumerate(splits):\n",
    "        train_keys = all_keys[train_idx]\n",
    "        test_keys = all_keys[test_idx]\n",
    "        if i == fold:\n",
    "            break\n",
    "    return train_keys, test_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adri/Studies/PhD_UCD/notebooks/batchgenerators/dataloading/data_loader.py:53: DeprecationWarning: This DataLoader will soon be removed. Migrate everything to SlimDataLoaderBase now!\n",
      "  warn(\"This DataLoader will soon be removed. Migrate everything to SlimDataLoaderBase now!\", DeprecationWarning)\n",
      "/home/adri/Studies/PhD_UCD/notebooks/batchgenerators/dataloading/data_loader.py:58: UserWarning: We currently strongly discourage using num_batches != None! That does not seem to work properly\n",
      "  warn(\"We currently strongly discourage using num_batches != None! That does not seem to work properly\")\n",
      "/home/adri/Studies/PhD_UCD/notebooks/batchgenerators/dataloading/data_loader.py:53: DeprecationWarning: This DataLoader will soon be removed. Migrate everything to SlimDataLoaderBase now!\n",
      "  warn(\"This DataLoader will soon be removed. Migrate everything to SlimDataLoaderBase now!\", DeprecationWarning)\n",
      "/home/adri/Studies/PhD_UCD/notebooks/batchgenerators/dataloading/data_loader.py:58: UserWarning: We currently strongly discourage using num_batches != None! That does not seem to work properly\n",
      "  warn(\"We currently strongly discourage using num_batches != None! That does not seem to work properly\")\n",
      "/home/adri/Studies/PhD_UCD/notebooks/batchgenerators/transforms/abstract_transforms.py:53: DeprecationWarning: This is deprecated. All applicable transfroms now have a p_per_sample argument which allows batchgenerators to do or not do an augmentation on a per-sample basis instead of the entire batch\n",
      "  DeprecationWarning)\n",
      "/home/adri/anaconda3/envs/python3.6/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "LR 0.0001\n",
      "tensor(0.7370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "train: bce: 0.382130, dice: 0.756698, loss: 0.569414\n",
      "tensor(0.4667, device='cuda:0')\n",
      "val: bce: 0.181692, dice: 0.705315, loss: 0.443503\n",
      "saving best model\n",
      "1m 4s\n",
      "Epoch 1/4\n",
      "----------\n",
      "LR 0.0001\n",
      "tensor(0.4586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "train: bce: 0.165456, dice: 0.682150, loss: 0.423803\n",
      "tensor(0.4033, device='cuda:0')\n",
      "val: bce: 0.161305, dice: 0.660609, loss: 0.410957\n",
      "saving best model\n",
      "1m 39s\n",
      "Epoch 2/4\n",
      "----------\n",
      "LR 0.0001\n",
      "tensor(0.3925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "train: bce: 0.161996, dice: 0.628321, loss: 0.395158\n",
      "tensor(0.3846, device='cuda:0')\n",
      "val: bce: 0.143687, dice: 0.583600, loss: 0.363643\n",
      "saving best model\n",
      "1m 41s\n",
      "Epoch 3/4\n",
      "----------\n",
      "LR 0.0001\n",
      "tensor(0.3088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "train: bce: 0.117665, dice: 0.511735, loss: 0.314700\n",
      "tensor(0.2980, device='cuda:0')\n",
      "val: bce: 0.126083, dice: 0.482696, loss: 0.304389\n",
      "saving best model\n",
      "1m 45s\n",
      "Epoch 4/4\n",
      "----------\n",
      "LR 0.0001\n",
      "tensor(0.2206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "train: bce: 0.123123, dice: 0.462289, loss: 0.292706\n",
      "tensor(0.2596, device='cuda:0')\n",
      "val: bce: 0.107468, dice: 0.451970, loss: 0.279719\n",
      "saving best model\n",
      "1m 46s\n",
      "Best val loss: 0.279719\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from preprocessing import load_dataset\n",
    "import unet\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = cf['num_classes']\n",
    "\n",
    "opts = {'duke_unet_opts':{\"n_input_channels\":cf['n_input_channels'],\"n_classes\":cf['num_classes']}}\n",
    "\n",
    "model = unet.Unet(opts).to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n",
    "\n",
    "\n",
    "fold = 0\n",
    "train_keys, test_keys = get_split(fold)\n",
    "train_data = load_dataset(train_keys)\n",
    "\n",
    "data = load_dataset()\n",
    "nb_entities = len(data)\n",
    "types = [x for x in data[1].keys() if 'data' in x.split('_')]\n",
    "nb_samples = []\n",
    "for typ in types:\n",
    "    nb_samples.append([np.shape(data[x][typ])[0] for x in range(1,nb_entities+1)])\n",
    "max_samples = max(nb_samples[0]+nb_samples[1])\n",
    "min_samples = min(nb_samples[0]+nb_samples[1])\n",
    "avg_samples = np.ceil(np.mean(nb_samples[0]+nb_samples[1]))\n",
    "all_samples_without_transformation = nb_entities * len(types) * avg_samples\n",
    "ALL_SAMPLES = all_samples_without_transformation\n",
    "NUM_BATCHES = np.ceil(ALL_SAMPLES/BATCH_SIZE)\n",
    "NUM_BATCHES = 100\n",
    "\n",
    "val_data = load_dataset(test_keys)\n",
    "\n",
    "data_gen_validation = BatchGenerator_2D(val_data, BATCH_SIZE, num_batches=NUM_BATCHES, seed=False,\n",
    "                                        PATCH_SIZE=INPUT_PATCH_SIZE)\n",
    "data_gen_validation = MultiThreadedAugmenter(data_gen_validation,\n",
    "                                             ConvertSegToOnehotTransform(range(num_classes), 0, \"seg_onehot\"),\n",
    "                                             1, 2, [0])\n",
    "data_gen_training = create_data_gen_train(train_data, cf['BATCH_SIZE'], cf['INPUT_PATCH_SIZE'], NUM_BATCHES, cf['num_classes'])\n",
    "dataloaders = {'train': data_gen_training, 'val': data_gen_validation}\n",
    "model, train_history, val_history = train_model(model, optimizer_ft, exp_lr_scheduler, dataloaders, num_epochs=cf['n_epochs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "torch.save(model, './entire_model_' + now.strftime(\"%Y-%m-%d_%H-%M-%S\") + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some inference and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 1.96 GiB total capacity; 897.73 MiB already allocated; 25.56 MiB free; 32.27 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-144b56419280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out.nii.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Studies/PhD_UCD/notebooks/unet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# down layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv1_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv1_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv3_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv3_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Studies/PhD_UCD/notebooks/unet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 1.96 GiB total capacity; 897.73 MiB already allocated; 25.56 MiB free; 32.27 MiB cached)"
     ]
    }
   ],
   "source": [
    "data_dict = next(data_gen_training)\n",
    "inputs = torch.from_numpy(data_dict['data']).to(device)\n",
    "outputs = model(inputs)\n",
    "outputs.cpu().detach().numpy().to_filename('out.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations(model, loader, layer_fun=None, n_sample=10):\n",
    "    \"\"\"\n",
    "    Return Activations\n",
    "\n",
    "    :param model: An nn.Module object whose activations we want to inspect.\n",
    "    :param loader: A generator whose next() iterator returns (x, y).\n",
    "    :param layer_fun: A function that returns a tensor of features, when\n",
    "      applied to a model and input x.\n",
    "    :param n_sample: The number of images from the loader to compute\n",
    "      activations for.\n",
    "    :returns summary: A list of dictionaries, each giving the image patch,\n",
    "      feature activations for the first n_sample patches in\n",
    "      the loader.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> loader = patch_gen_train()\n",
    "    >>> model = UNet(params[\"model_opts\"])\n",
    "    >>> model.load_state_dict(\n",
    "          torch.load(model_path)\n",
    "        )\n",
    "    >>>\n",
    "    >>> summary = activations(model, loader)\n",
    "    \"\"\"\n",
    "    if not layer_fun:\n",
    "        layer_fun = lambda model, x: model.pre_pred(x)\n",
    "\n",
    "    summary = []\n",
    "    for i, D in enumerate(loader):\n",
    "        print(\"batch {}\".format(i))\n",
    "        if i > n_sample:\n",
    "            break\n",
    "\n",
    "        # each element of the batch\n",
    "        x = torch.from_numpy(D['data']).to(device)\n",
    "        types = D['types']\n",
    "        patient_ids = D['patient_ids']\n",
    "        pathologies = D['pathologies']\n",
    "        seg_onehot = D['seg_onehot']\n",
    "        y = torch.from_numpy(D['seg_onehot']).to(device) # we need an output encoded into 4 layers          \n",
    "        \n",
    "        h = layer_fun(model, x)\n",
    "        for j, _ in enumerate(x):\n",
    "            summary.append({\n",
    "                \"x\": x[j].detach().cpu().numpy(),\n",
    "                \"h\": h[j].detach().cpu().numpy(),\n",
    "            })\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def ft_fun(model, x):\n",
    "    \"\"\"\n",
    "    Activations to write for the U-net\n",
    "    \"\"\"\n",
    "    x, conv1_out, conv1_dim = model.down_1(x)\n",
    "    x, conv2_out, conv2_dim = model.down_2(x)\n",
    "    x, conv3_out, conv3_dim = model.down_3(x)\n",
    "    x, conv4_out, conv4_dim = model.down_4(x)\n",
    "\n",
    "    # norms for each feature map at the encoder layer\n",
    "    x = model.conv5_block(x)\n",
    "    return torch.sum(x ** 2, dim=(2, 3)) ** (0.5)\n",
    "\n",
    "def reshape_activations(summary):\n",
    "    \"\"\"\n",
    "    Matrix-ify the output of activations()\n",
    "\n",
    "    :param summary: The output of the activations function. This is a list of\n",
    "      dictionaries describing each patch.\n",
    "    :return: A tuple with the following components,\n",
    "      H: A numpy array whose rows correspond to patches and whose values are\n",
    "      activations.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> summary = activations(model, loader)\n",
    "    >>> H = reshape_activations(summary)\n",
    "    >>>\n",
    "    >>> np.savetxt(os.path.join(params[\"save_dir\"], \"H.csv\"), H, delim=\",\")\n",
    "    \"\"\"\n",
    "    N = len(summary)\n",
    "    K = len(summary[0][\"h\"])\n",
    "\n",
    "    H = np.zeros((N, K))\n",
    "\n",
    "\n",
    "    for i in range(N):\n",
    "        H[i] = summary[i][\"h\"]\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.96 GiB total capacity; 916.57 MiB already allocated; 25.56 MiB free; 13.43 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-be9ab38ccdf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mft_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"./results_experiment/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mn_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-26-be9ab38ccdf8>\u001b[0m in \u001b[0;36msave_summary\u001b[0;34m(model, loader, layer_fun, out_path, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}-H.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-cdd4fd959e87>\u001b[0m in \u001b[0;36mactivations\u001b[0;34m(model, loader, layer_fun, n_sample)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# each element of the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'types'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpatient_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'patient_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.96 GiB total capacity; 916.57 MiB already allocated; 25.56 MiB free; 13.43 MiB cached)"
     ]
    }
   ],
   "source": [
    "def save_summary(model, loader, layer_fun, out_path, **kwargs):\n",
    "    summary = activations(model, loader, layer_fun, n_sample=10)\n",
    "    H = reshape_activations(summary)\n",
    "    np.savetxt(\"{}-H.csv\".format(out_path), H, delimiter=\",\")\n",
    "\n",
    "# save activations to file\n",
    "save_summary(\n",
    "    model,\n",
    "    data_gen_training,\n",
    "    ft_fun,\n",
    "    \"./results_experiment/\",\n",
    "    n_sample=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate time that it will take to train the model\n",
    "\n",
    "- How much data samples do we have?\n",
    "- How much time does it take to process one batch?\n",
    "- How many epochs does it suppose to run?\n",
    "- How much time does it take to process one epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much data samples do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_entities = len(data)\n",
    "types = [x for x in data[1].keys() if 'data' in x.split('_')]\n",
    "nb_samples = []\n",
    "for typ in types:\n",
    "    nb_samples.append([np.shape(data[x][typ])[0] for x in range(1,nb_entities+1)])\n",
    "max_samples = max(nb_samples[0]+nb_samples[1])\n",
    "min_samples = min(nb_samples[0]+nb_samples[1])\n",
    "avg_samples = np.ceil(np.mean(nb_samples[0]+nb_samples[1]))\n",
    "all_samples_without_transformation = nb_entities * len(types) * avg_samples\n",
    "ALL_SAMPLES = all_samples_without_transformation\n",
    "NUM_BATCHES = np.ceil(ALL_SAMPLES/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} patients in the dataset.\".format(nb_entities))\n",
    "print(\"There are {} types of data per patient.\".format(len(types)))\n",
    "for typ in types:\n",
    "    print(\"There are {} samples of type: {}\".format(np.shape(data[1][typ])[0], typ))\n",
    "print('There are around {} number of samples without transformation'.format(all_samples_without_transformation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much time does it take to process one batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many epochs does it suppose to run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much time does it take to process one epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11m 10.0s | On avarage it takes 11 minutes and 10.0 seconds to train one epoch.\n"
     ]
    }
   ],
   "source": [
    "# with batch size = 1\n",
    "times = ['9m 45s', '11m 28s', '11m 28s', '11m 32s', '11m 36s']\n",
    "minutes = [int(x.split('m')[0])*60 for x in times]\n",
    "seconds = [int(x.split(' ')[1].split('s')[0]) for x in times]\n",
    "times_sec = []\n",
    "for i in range(len(minutes)):\n",
    "    times_sec.append((minutes[i] + seconds[i])/60)\n",
    "mean = np.mean(times_sec)\n",
    "print(\"{0}m {1}s | On avarage it takes {0} minutes and {1} seconds to train one epoch.\".format(int(mean),np.ceil((mean - int(mean))*60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"It takes 9m 45s to process one epoch of batch size with 1 sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With the following transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "\n",
    "##### Decisions to be made:\n",
    "\n",
    "- What transformations to use?\n",
    "- How big the batch should be?\n",
    "- For how many epochs it shoud train for?\n",
    "- How many batches should be in one epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO:\n",
    "\n",
    "- why the heck, the images are rotated and scaled in the batches even with transforms off? - investigate\n",
    "- correct collected intermediate metrics +\n",
    "- collect activations from the ordered batch generator <br> to make it more meaningful to analyze\n",
    "- single patient data input +\n",
    "- assess model performance +\n",
    "- extract training parameters into a separate file +\n",
    "- collect training history +\n",
    "- save training history to the file +\n",
    "- train the network +\n",
    "- collect activations from the bottleneck layer +\n",
    "- collect adequate patches +\n",
    "- plot results +\n",
    "- submission file for ACDC17 format: patientXXX_ED.nii.gz and patientXXX_ES.nii.gz, https://acdc.creatis.insa-lyon.fr/#phase/5846c3ab6a3c7735e84b67f2/submit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Links:\n",
    "\n",
    "- https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html\n",
    "- https://acdc.creatis.insa-lyon.fr/#phase/5846c3ab6a3c7735e84b67f2/submit\n",
    "- https://www.creatis.insa-lyon.fr/Challenge/acdc/evaluationSegmentation.html\n",
    "- https://www.creatis.insa-lyon.fr/Challenge/acdc/code/metrics_acdc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Isensee et al.:\n",
    "\"The 3D model was trained for 300 epochs in a 5-fold cross validation using the\n",
    "ADAM solver and a pixel-wise categorical cross-entropy loss. The initial learning\n",
    "rate of 5  104 was decayed by 0.98 per epoch, where an epoch was defined as\n",
    "100 batches, each comprising four training examples. Training examples were\n",
    "generated as random crops of size 224  224  10 voxels taken from a randomly\n",
    "chosen training patient and phase instance (ED/ES).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
